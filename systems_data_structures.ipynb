{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monetary-recovery",
   "metadata": {},
   "source": [
    "# Project 1 - Building the Backend of an AirBnB Website\n",
    "\n",
    "The growth of the homesharing and short-term rental markets has presented opportunities and challenges for communities globally. While for some it encourages tourism and provides additional income streams, for others it exacerbates affordable housing shortages. You decide you can create a website sharing the actual data. In this project you will build the \"backend\" for that website. The following tasks will guide you to experiment with the optimal data structures to store information about listings and their reviews, consider the algorithmic choices you might make to traverse those data structures and even explore the underlying data types you will store the data as. Your goal is to make all of the programs as efficient as possible!\n",
    "\n",
    "In this project, you will be given a handful of functionalities you are expected to implement for the backend of the website. For each functionality you will use, upgrade or build a data structure to store the data you need for that functionality and determine how to traverse that data efficiently to serve information on your website.\n",
    "\n",
    "**General note**: You may notice there are several code cells with the tag `excluded_from_script`. These code cells will not be run by the autograder, and you should make sure to preserve these tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8ea7c-8cde-436b-85e6-91b46e7ba55f",
   "metadata": {},
   "source": [
    "We start by implementing relevant packages. Note that you do not need to use any NumPy, Pandas or Sklearn functions in this project; they are only imported to set up the dataset and local test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "second-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanvimurke/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random, string, hashlib, re, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from predict_rating import predict\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23236f1b",
   "metadata": {},
   "source": [
    "To start, run the following cell to download the data file. Note that this only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9317458",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://clouddatascience.blob.core.windows.net/s21-foundation-data-science/systems_data_structures/test_cities.csv'\n",
    "r = requests.get(url)\n",
    "with open('test_cities.csv', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hindu-coral",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/48d4h_554811pcq9nyp287340000gn/T/ipykernel_91746/1284643275.py:1: DtypeWarning: Columns (0,1,3,22,28,29,41,45,46,54,61,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  airbnb_data = pd.read_csv('test_cities.csv')\n"
     ]
    }
   ],
   "source": [
    "airbnb_data = pd.read_csv('test_cities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c5e11-d233-4e34-a881-679f243e5f4a",
   "metadata": {},
   "source": [
    "You may see a `DTypeWarning` from the above cell, but you don't need to worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-moisture",
   "metadata": {},
   "source": [
    "## Question 1: Analyzing runtime performance and memory requirements of different datatypes (5 points)\n",
    "\n",
    "One functionality that we want is the ability to predict a listing's price based on its features (e.g., locations, amenities). In addition, this feature should be available on the user's phones, which have limited memory and processing capabilities. We have the option to store our data and model in one of three types: 16-bit floating point, 32-bit floating point, or 64-bit floating point. We also have three criteria to evaluate each option:\n",
    "\n",
    "1. How much memory does it take to store the model?\n",
    "1. How long does it take to perform model prediction?\n",
    "1. How good is the model's prediction?\n",
    "\n",
    "To answer these questions, let's try out each of our datatype option on a synthetic dataset. Here we assume that a linear regression model is used to carry out the prediction.\n",
    "\n",
    "Run the code cell below; while you don't need to know the specifics of the NumPy operations that are used, make sure you understand the high-level role of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef5e4794-d72a-4e83-a480-feeeff8492e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    \"\"\"\n",
    "    Generate the input matrix data (X) and output label vector (y) for a regression problem\n",
    "    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details\n",
    "    \"\"\"\n",
    "    print(\"I am now executing construct_dataset\")\n",
    "    data = make_regression(n_samples=10000, random_state=42)\n",
    "    X, y = data[0], data[1]\n",
    "    X = np.concatenate((np.ones((len(X),1)), X), axis=1)\n",
    "    return X, y\n",
    "\n",
    "def train_linear_regression_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a linear regression model to predict the output label vector y based on the input data matrix X\n",
    "    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor\n",
    "    \"\"\"\n",
    "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return w\n",
    "\n",
    "def compute_memory_usage(model):\n",
    "    \"\"\"\n",
    "    Compute the number of bytes needed to store the model vector\n",
    "    \"\"\"\n",
    "    return model.nbytes\n",
    "\n",
    "def compute_prediction_runtime(X, model, N=1000):\n",
    "    \"\"\"\n",
    "    Record the average time taken to perform prediction, sampled from 1000 runs\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    for _ in range(N):\n",
    "        start = datetime.now()\n",
    "        y = X @ model\n",
    "        end = datetime.now()\n",
    "        total_time += (end - start).total_seconds()\n",
    "    return total_time / N\n",
    "\n",
    "def compute_prediction_error(X, model, y_true):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels\n",
    "    \"\"\"\n",
    "    y_predicted = X @ model\n",
    "    return np.mean((y_predicted - y_true)**2)\n",
    "\n",
    "def evaluate_dtype(dtype):\n",
    "    \"\"\"\n",
    "    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type\n",
    "    \"\"\"\n",
    "    # train the model on 60% of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)\n",
    "    model = train_linear_regression_model(X_train, y_train)\n",
    "    \n",
    "    # convert the test data and trained model vector to the specified dtype\n",
    "    X_test, model = X_test.astype(dtype), model.astype(dtype)\n",
    "    \n",
    "    # perform evaluation\n",
    "    memory_usage = compute_memory_usage(model)\n",
    "    prediction_runtime = compute_prediction_runtime(X_test, model)\n",
    "    prediction_error = compute_prediction_error(X_test, model, y_test)\n",
    "    print(f\"A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}\")\n",
    "    return memory_usage, prediction_runtime, prediction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff88f67-56cd-4e1f-a530-1b02cf786100",
   "metadata": {},
   "source": [
    "Now that the set up has finished, let's begin the evaluation! Run the code cell below, then report your finding in the `evaluate_data_type` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52bb521d-9e1c-44d2-94d0-36be42af9ae4",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am now executing construct_dataset\n",
      "A model stored in data type <class 'numpy.float16'> consumes 202 bytes, takes 0.0013350460000000042 seconds to perform prediction on average, and has a prediction error of 0.004490931025845336\n",
      "I am now executing construct_dataset\n",
      "A model stored in data type <class 'numpy.float32'> consumes 404 bytes, takes 2.6515000000000023e-05 seconds to perform prediction on average, and has a prediction error of 1.548874415705422e-10\n",
      "I am now executing construct_dataset\n",
      "A model stored in data type <class 'numpy.float64'> consumes 808 bytes, takes 0.00013619399999999785 seconds to perform prediction on average, and has a prediction error of 1.5046938421399573e-26\n"
     ]
    }
   ],
   "source": [
    "evaluate_dtype(np.float16);\n",
    "evaluate_dtype(np.float32);\n",
    "evaluate_dtype(np.float64);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-legend",
   "metadata": {},
   "source": [
    "Based on the printouts above, fill in the four variables `fastest`, `slowest`, `least_memory`, `best_test_err` in the `select_data_type` function. In particular,\n",
    "* Each variable should hold one of the three string values: `\"float16\"`, `\"float32\"`, `\"float64\"`.\n",
    "* `fastest` reports the datatype that yields the lowest **prediction** time\n",
    "* `slowest` reports the datatype that yields the highest **prediction** time\n",
    "* `least_memory` reports the datatype that yields the lowest number of bytes\n",
    "* `best_test_err` reports the datatype that has the lowest test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brief-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_data_type():\n",
    "    fastest = \"float32\"\n",
    "    slowest = \"float16\"\n",
    "    least_memory = \"float16\"\n",
    "    best_test_err = \"float64\"\n",
    "    \n",
    "    \n",
    "    return fastest, slowest, least_memory, best_test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e52590-914a-4fb1-a6d8-eda092e3795a",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_evaluate_data_type():\n",
    "    assert all(answer in [\"float16\", \"float32\", \"float64\"] for answer in evaluate_data_type())\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_evaluate_data_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-idaho",
   "metadata": {},
   "source": [
    "Now you have a good understanding of the trade-offs between different data types. Here are some other points on this topic to think about:\n",
    "* Note that in our implementation of `evaluate_dtype`, the data type conversion only takes place after the linear regression model has been trained. The reason is that model training happens on our side, so we are typically not too concerned about memory or runtime constraints. However, once a model has been trained, it will be deployed to the client side, which could be a mobile phone or a smart watch with very limited resources; in this case, choosing the appropriate data type to store the model becomes more important. This technique is called [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization).\n",
    "* Were you surprised about the data type that led to the slowest inference time? Why may this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-bundle",
   "metadata": {},
   "source": [
    "## Question 2: Filtering user ids given blacklist and whitelist\n",
    "\n",
    "Some AirBnB users exhibit bad behaviors (e.g., they are rude to their hosts) and are banned from the platform. The IDs of these users are stored in the text file `blacklist.txt`. We also have a file `whitelist.txt`, which stores the ids of users that are not banned. If the same user id appears in both `blacklist.txt` and `whitelist.txt`, the white list will take priority, i.e., that user is **not** banned. If the input user ID isn’t in the blacklist and isn’t in the whitelist, it will be mapped to `False` (that user is not banned).\n",
    "\n",
    "Your task is to implement a filter function that, given an input list of user IDs, maps each ID to the boolean `True` if it belongs to a banned user, and `False` otherwise. Note that this involves two sub-tasks:\n",
    "1. Selecting appropriate data structures to store the blacklisted and whitelisted user IDs.\n",
    "1. Perform the mapping based on these data structures.\n",
    "\n",
    "We provide the starting code for sub-task 1 in the function `read_blacklist_and_whitelist` below, which saves the ids into two Python lists. **You should modify this code to store the data more efficiently, for the purpose of performing sub-task 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "usual-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blacklist_and_whitelist(blacklist_file, whitelist_file):\n",
    "    '''\n",
    "    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs\n",
    "\n",
    "    Reads the blacklist and whitelist from input files and store them into appropriate data structures\n",
    "    \n",
    "    args:\n",
    "        blacklist_file (str) : file path of blacklist, each line is separate id\n",
    "        whitelist_file (str) : file path of whitelist, each line is separate id\n",
    "        \n",
    "    returns: Tuple(blacklist, whitelist)\n",
    "        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs\n",
    "        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs\n",
    "    '''\n",
    "    blacklist = set()\n",
    "    whitelist = set()\n",
    "    \n",
    "    with open(blacklist_file, \"r\") as f:\n",
    "        blacklist = set(int(x.strip()) for x in f.readlines())\n",
    "    \n",
    "    with open(whitelist_file, \"r\") as f:\n",
    "        whitelist = set(int(x.strip()) for x in f.readlines()) \n",
    "    \n",
    "    return blacklist, whitelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1d628-43de-4f44-b69c-76ae230dfdee",
   "metadata": {},
   "source": [
    "We now store the returned blacklist and whitelist as global variables, so that they can be accessed in later tasks. If you later change your implementation of `read_blacklist_and_whitelist`, make sure to rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nominated-praise",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "blacklist, whitelist = read_blacklist_and_whitelist(\"blacklist.txt\", \"whitelist.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-encoding",
   "metadata": {},
   "source": [
    "### Question 2.1: Checking if a single user is banned (5 points)\n",
    "Now let's move on to sub-task 2; we will consider a simple case first. Implement the function `check_single_id` that checks for whether a single user ID is banned or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "through-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_single_id(id_to_check, blacklist, whitelist):\n",
    "    '''\n",
    "    Checks whether an input ID is banned, based on the stored blacklist and whiteliist\n",
    "    \n",
    "    args:\n",
    "        id_to_check (int) : the user ID that you need to check\n",
    "        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs\n",
    "        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs\n",
    "    \n",
    "    returns:\n",
    "        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise\n",
    "    '''\n",
    "    if id_to_check in whitelist:\n",
    "        id_state = False\n",
    "    elif id_to_check in blacklist:\n",
    "        id_state = True\n",
    "    else:\n",
    "        id_state = False\n",
    "    return id_state\n",
    "    #raise NotImplementedError(\"Complete this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afraid-candy",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "42.2 ns ± 11.8 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def test_check_single_id():\n",
    "    blacklist, whitelist = read_blacklist_and_whitelist(\"blacklist.txt\", \"whitelist.txt\")\n",
    "    assert check_single_id(59735, blacklist, whitelist) == False, \"Check that you handle cases when id is in both blacklist and whitelist!\"\n",
    "    assert check_single_id(5935, blacklist, whitelist) == True, \"Check that you handle cases when id is in blacklist and not in whitelist!\"\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_check_single_id()\n",
    "\n",
    "# let's also see how long it takes to run this function\n",
    "%timeit check_single_id(59735, blacklist, whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-gossip",
   "metadata": {},
   "source": [
    "### Question 2.2: Filtering list of ids (10 points)\n",
    "Now we move on to the real filtering task. Implement the function `check_list_ids` which, given an input list of IDs, maps each ID to the boolean `True` if it belongs to a banned user, and `False` otherwise. You can either reuse your implementation of `check_single_id`, or implement this task from scratch. Note that the input list of IDs may have very large size, so make sure you do not perform any redundant or repeated operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "interesting-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_ids(input_ids, blacklist, whitelist):\n",
    "    '''\n",
    "    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist\n",
    "    \n",
    "    args:\n",
    "        input_ids (List[int]) : the user ID that you need to check\n",
    "        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs\n",
    "        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs\n",
    "    \n",
    "    returns:\n",
    "        List[bool] : a list having the same length as input_ids, where the entry at index i\n",
    "            is True if input_ids[i] is banned, and False otherwise\n",
    "    '''\n",
    " \n",
    "    '''\n",
    "    check_dict = {ids: check_single_id(ids, blacklist, whitelist) for ids in input_ids}\n",
    "    check_list = list(check_dict.values())\n",
    "    return check_list\n",
    "    '''\n",
    "\n",
    "    return [check_single_id(ids, blacklist, whitelist) for ids in input_ids]\n",
    "    \n",
    "\n",
    "    #raise NotImplementedError(\"Complete this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "subtle-happening",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "57.6 ms ± 3.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def test_check_list_ids():\n",
    "    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]\n",
    "    ref = [False, True, True, False, True, False, False, False, False, False]\n",
    "    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_check_list_ids()\n",
    "\n",
    "# let's also see how long it takes to run this function\n",
    "random.seed(42)\n",
    "test_input_ids = [random.randint(0, 100000) for x in range(1000000)]\n",
    "%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb7594-1f28-46aa-9e77-d09bf34b4a46",
   "metadata": {},
   "source": [
    "The autograder will test your code on an input list of roughly 100000 IDs as well, so make sure everything is optimized!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-eleven",
   "metadata": {},
   "source": [
    "## Question 3: High performance FIFO storage system (10 points)\n",
    "Next, you want to *dynamically* display the descriptions of AirBnB homes to the user. Imagine that the user enters a search query to AirBnb and gets an initial collection of search results. Then, the user may choose to alter their search query in some ways (for example, changing the start and end date of their stay), causing some new listings to be inserted to the search results, and some old listings to be removed. Your task is to identify a data structure which can efficiently perform these insertion/removal operations. To simplify the context, we will further assume that, when a removal operation takes place, only the oldest element in the collection is removed (oldest in the sense that it was added to the collection before any other element) -- in other words, your collection of search results behaves in a First In First Out (FIFO) manner.\n",
    "\n",
    "In the cell below, we provide a starting implementation of the [generator function](https://wiki.python.org/moin/Generators) `Storage`, which takes as input an initial list of AirBnB homes `initial_data`, as well as a data socket `data_socket`. `data_socket` is a Python generator that iteratively yields the next operation which should be performed on `initial_data`:\n",
    "* If `data_socket` yields the string `\"generate\"`, the oldest element in `initial_data` should be removed and yielded by your function, unless `initial_data` is currently empty. You can assume the elements in `initial_data` are in the same order that you would need to yield them -- the oldest element is at the beginning of the list.\n",
    "* If `data_socket` yields any other string `x`, insert `x` to `initial_data`.\n",
    "\n",
    "The current implementation simply operates directly on the list `initial_data`, which may not be the most efficient approach (recall that removing from the head of a list is costly). Your task is to move the elements of `initial_data` to a more suitable data structure and reimplement the insertion/removal operations accordingly on this new data structure.\n",
    "\n",
    "**Example:** let's say `initial_data = [\"10\", \"20\"]` and `data_socket` yields the following strings: `\"30\", \"generate\", \"generate\", \"generate\", \"generate\", \"40\", \"generate\", \"50\"`. Then you should:\n",
    "1. Add `\"30\"` to `initial_data`\n",
    "1. Remove and yield the four oldest elements in `initial_data`, if they exist.\n",
    "1. Add `\"40\"` and `\"50\"` to `initial_data`.\n",
    "1. Finally, `Storage(initial_data, data_socket)` becomes a generator that yields `\"10\", \"20\", \"30\", \"40\"` (because `\"10\"` was removed from `initial_data` first, followed by `\"20\", \"30\", \"40\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "educational-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Storage(initial_data, data_socket):\n",
    "    '''\n",
    "    NOTE: you should modify this function to be more efficient\n",
    "    \n",
    "    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data\n",
    "    \n",
    "    args:\n",
    "        initial_data (List[Object]) - the initial collection of search results\n",
    "        data_socket (generator [Object|String]) - a generator that yields either some object or the string \"generate\"\n",
    "        \n",
    "    yields:\n",
    "        Object from initial_data or data_socket\n",
    "    '''\n",
    "    fifo_storage = deque(initial_data)\n",
    "    for item in data_socket():\n",
    "        if item == \"generate\":\n",
    "            if len(fifo_storage) > 0:\n",
    "                yield fifo_storage.popleft()\n",
    "        else:\n",
    "            fifo_storage.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "durable-craft",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "40 ms ± 3.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def test_storage():\n",
    "    def test_socket():\n",
    "        for x in [3,\"generate\",\"generate\",\"generate\",\"generate\",4,\"generate\",5]:\n",
    "            yield x\n",
    "\n",
    "    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_storage()\n",
    "\n",
    "# let's also see how long it takes to run this function\n",
    "def data_socket():\n",
    "    # a sample data_socket that yields \"generate\" once in every 10 elements\n",
    "    for i in range(10000):\n",
    "        if i % 10: \n",
    "            yield \"generate\"\n",
    "        yield airbnb_data[\"description\"].iloc[i % len(airbnb_data)]\n",
    "        \n",
    "%timeit list(Storage(list(airbnb_data[\"description\"].values), data_socket));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-extension",
   "metadata": {},
   "source": [
    "## Question 4: Finding neighborhood spans (25 points)\n",
    "\n",
    "To help users navigate the website conveniently, we want to add hyperlinks to the neighborhoods that are mentioned in the housing descriptions. For example, given the description string `\"It's a beautiful house in Salt Lake District and you will enjoy it here!\"`, we can identify `Salt Lake District` as a reference to a neighborhood, so we can embed it in a hyperlink. In this way, what the user will see on our website is\n",
    "\n",
    "> It's a beautiful house in [Salt Lake District](https://en.wikipedia.org/wiki/Salt_Lake_City_School_District) and you will enjoy it here!\n",
    "\n",
    "and they can click on the hyperlink to know more about Salt Lake District. In our context we will not care about where the hyperlink points to, but we do want to identify the exact span that should be embedded in the link. As you can see from the above example, the hyperlink starts at `Salt` and ends at `District`.\n",
    "\n",
    "But how do we know which word, or collection of words, corresponds to a neighborhood name? Note that our dataset has a column `host_neighbourhood`, which lists the neighborhood of every AirBnB home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2b0b62-bbdc-4ed5-b09f-545fbae64e95",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              Indische Buurt\n",
       "1              Grachtengordel\n",
       "2              Grachtengordel\n",
       "3         Westelijke Eilanden\n",
       "4           Amsterdam Centrum\n",
       "                 ...         \n",
       "243009                    NaN\n",
       "243010                    NaN\n",
       "243011                    NaN\n",
       "243012               Østerbro\n",
       "243013                    NaN\n",
       "Name: host_neighbourhood, Length: 243014, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_data[\"host_neighbourhood\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c628e-be5d-40c2-83ab-f6f1efd6bc3b",
   "metadata": {},
   "source": [
    "So we can collect all the unique entries from this series and preprocess them to form our neighborhood vocabulary. Your task is to identify a suitable data structure for this vocabulary and construct it in the function `build_neighborhood_vocab` below. Then implement the function `find_spans` which, given a housing description and the neighborhood vocabulary, identifies all the hyperlink spans. Each hyperlink span is a tuple with the format `(start_index, end_index, neighborhood_name)`, where the `start_index` and `end_index` denote the index of the neighborhood name's starting word and ending word within the description string.\n",
    "\n",
    "As an example, with the input string description `\"It's a beautiful house in Salt Lake District and you will enjoy it here!\"`, the expected output would be `[(6, 8, \"salt lake district\")]`. Although `salt` is at index 5 and `district` at index 7 in the description, note that \"It's\" gets preprocessed to \"It\" and \"s\". Thus, `It` and `s` are two separate words in this description. Sentece get's transformed to `\"It s a beautiful house in Salt Lake District and you will enjoy it here!\"` after preprocessing the description text.\n",
    "\n",
    "**Notes for `build_neighborhood_vocav`**:\n",
    "* The neighborhood names need to be preprocessed before they are added to the vocabulary. We have provided the implementation of this preprocessing task for you in the function `preprocess_name`. You should not modify this function.\n",
    "* We also provide the function `check_valid_name`, which returns `True` if a neighborhood name, after preprocessing, is valid (i.e., it should not be `nan` and should have more than five characters). Only valid neighborhood names should be added to the vocabulary.\n",
    "* You may find the [trie](https://en.wikipedia.org/wiki/Trie) data structure useful in this task.\n",
    "\n",
    "**Notes for `find_spans`**:\n",
    "* There are some text preprocessing tasks to perform on the description string; these have been implemented for you at the start of `find_spans` and should not be modifiied.\n",
    "* Make sure to find all spans; if there are two spans of the same neighborhood, you need to output both.\n",
    "* For `start_index` and `end_index`, note that indexing starts at 0 and `end_index` is inclusive. If the identified neighborhood consists of a single word, then `start_index` is equal to `end_index`.\n",
    "* If two spans are overlapping but have different `start_index`, output both of them. For example, let's say our vocabulary has two neighborhoods: `\"Great district\"` and `\"District of Learning\"`. If the input description is `\"great district of learning\"`, the output of `find_spans` should be `[(0,1,\"great district\"), (1,3,\"district of learning\")]`.\n",
    "* If two spans are overlapping and have the same `start_index` (i.e., one span is contained in another), only output the smaller span. For example, let's say our vocabulary has two neighborhoods: `\"District\"` and `\"District of Learning\"`. If the input description is `\"District of Learning is great!\" `, the output of `find_spans` should be `[(0, 0, \"district\")]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "20771c4f-40f0-4535-9148-5be9812dd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.isCompletedWord = False\n",
    "        \n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        \n",
    "    def insert(self, word):\n",
    "        current = self.root\n",
    "        for letter in word:\n",
    "            if letter not in current.children:\n",
    "                current.children[letter] = TrieNode()\n",
    "            current = current.children[letter]\n",
    "        current.isCompletedWord = True\n",
    "            \n",
    "    def search(self, word):\n",
    "        current = self.root\n",
    "        for letter in word:\n",
    "            if letter not in current.children:\n",
    "                return False\n",
    "            current = current.children[letter]\n",
    "        return current.isCompletedWord\n",
    "    \n",
    "    def startsWith(self, word):\n",
    "        current = self.root\n",
    "        for letter in word:\n",
    "            if letter not in current.children:\n",
    "                return False\n",
    "            current = current.children[letter]\n",
    "        return True   \n",
    "        \n",
    "def preprocess_name(name):\n",
    "    \"\"\"Perform some text cleaning on the neighborhood name\"\"\"\n",
    "    if str(name) == \"nan\":\n",
    "        return None\n",
    "    \n",
    "    # convert to lowercase and remove \"-\"\n",
    "    name = name.lower().replace(\"-\", \" \")\n",
    "    \n",
    "    # replace occurences of multiple spaces with a single space\n",
    "    return re.sub(r\"\\s+\", \" \", name)\n",
    "\n",
    "def check_valid_name(processed_name):\n",
    "    \"\"\"Check if a neighborhood name is valid\"\"\"\n",
    "    return processed_name is not None and len(processed_name) > 5\n",
    "\n",
    "def build_neighborhood_vocab(neighborhoods):\n",
    "    \"\"\"\n",
    "    Construct a vocabulary of processed and valid neighborhood names\n",
    "    \n",
    "    args:\n",
    "        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset\n",
    "    \n",
    "    returns:\n",
    "        collections : a data structure that stores the neighborhood names\n",
    "    \"\"\"\n",
    "    trie = Trie()\n",
    "    neighbor_set = set()\n",
    "    for word in neighborhoods:\n",
    "        processed_name = preprocess_name(word)\n",
    "        if check_valid_name(processed_name):\n",
    "            neighbor_set.add(processed_name)\n",
    "    for neighbor in neighbor_set:\n",
    "        trie.insert(neighbor)\n",
    "    return trie\n",
    "         \n",
    "    #raise NotImplementedError(\"Implement this task\")\n",
    "    \n",
    "def find_spans(description, neighborhood_vocab):\n",
    "    \"\"\"\n",
    "    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary\n",
    "    \n",
    "    args:\n",
    "        description (str) : a string description of an AirBnB home\n",
    "        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names\n",
    "    \"\"\"\n",
    "    # preprocess the description text, do not modify this code\n",
    "    if str(description) == \"nan\":\n",
    "        return []\n",
    "    description = description.lower()\n",
    "    for p in string.punctuation:\n",
    "        description = description.replace(p, \" \")\n",
    "    description = re.sub(r\"\\s+\", \" \", description)\n",
    "       \n",
    "    spans = []\n",
    "    desc_words = description.split()\n",
    "    index = 0 \n",
    "    neighbor_found = False\n",
    "    \n",
    "    while index < len(desc_words):\n",
    "        if neighborhood_vocab.startsWith(desc_words[index]):\n",
    "            start_index = index\n",
    "            while index < len(desc_words): \n",
    "                next_word = ' '.join(desc_words[start_index:index+1])\n",
    "                if not neighborhood_vocab.startsWith(next_word):\n",
    "                    break\n",
    "                if neighborhood_vocab.search(next_word):\n",
    "                    end_index = index\n",
    "                    neighbor_found = True\n",
    "                index += 1\n",
    "            if neighbor_found:\n",
    "                word_span = ' '.join(desc_words[start_index:end_index+1])\n",
    "                spans.append((start_index, end_index, word_span))\n",
    "                neighbor_found = False\n",
    "            index = start_index + 1\n",
    "        else:\n",
    "            index += 1\n",
    "    return spans\n",
    "            \n",
    "\n",
    "    # fill in your implementation here\n",
    "    #raise NotImplementedError(\"Implement this task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2adb0e48-63d8-454b-9a3c-9f271dd7becd",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "All tests passed!\n",
      "134 ms ± 53.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def test_find_spans():\n",
    "    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)\n",
    "    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]\n",
    "    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],\n",
    "           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]\n",
    "    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), \"Check your implementation!\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "%timeit test_find_spans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-session",
   "metadata": {},
   "source": [
    "## Question 5: Depth-first search (10 points)\n",
    "In this question, we will consider the referral feature: user `A` can refer a friend `B` to AirBnB and, if `B` joins, both `A` and `B` get some AirBnB credit. Assume we have a database of referrals, organized in the form of a poly-tree, where each node corresponds to a user. Node `A` is the parent of node `B` (i.e., there is a directed edge from `A` to `B`) if `B` was referred by `A`. Each node can have at most one parent, and some nodes may not have any children.\n",
    "\n",
    "Given a user ID, we want to identify the chain of referrals that led to this user joining the app. For this purpose, we will use depth-first search (DFS):\n",
    "1. Visit the first unvisited child node of a current node; in this case we would define the \"first\" child as the leftmost child.\n",
    "1. If all nodes were visited or if a node is a leaf (has no child nodes), go up.\n",
    "1. As soon as you find the target node, stop iterating.\n",
    "\n",
    "For example, given the following poly-tree:\n",
    "\n",
    "![](polytree.png)\n",
    "\n",
    "DFS would traverse the nodes in the following order: `1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11`. If the target node is `9`, we return the chain of ancestors of `9` as `[6, 7, 8, 9]` (note that `9` itself is also included in this list). In our context, we represent the above tree as a nested dictionary, where each node is a key that maps to all of its subtrees:\n",
    "\n",
    "```\n",
    "tree = {\n",
    "  '1 : {'2' : {}},\n",
    "  '3' : {'4' : {}, '5' : {}},\n",
    "  '6' : {'7' : {'8': {'9' : {}, '10' : {}}}},\n",
    "  '11' : {} \n",
    "}\n",
    "```\n",
    "\n",
    "Implement the function `dfs` that, given a target user ID and the referral database, computes (1) the chain of ancestors of the target node, and (2) the full DFS traversal sequence leading to the target node.\n",
    "\n",
    "**Notes**:\n",
    "* The target user ID may not be present in the dataset -- for this edge case, the chain of parents is an empty list, and the DFS traversal sequence should contain every node in the tree.\n",
    "* Each node should only appear at most once in the returned DFS traversal sequence, as well as the chain of parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "continental-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(target, tree):\n",
    "    '''\n",
    "    Find the chain of referrals leading to a target user ID\n",
    "    \n",
    "    args:\n",
    "        target (String) : the target user id\n",
    "        tree (dict) - the referral poly-tree represented in nested dictionary format\n",
    "\n",
    "    returns:\n",
    "        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent\n",
    "        history (List[String]) - the full DFS traversal sequence leading to the target node\n",
    "    '''\n",
    "    def dfs_recursive(target, tree, history, chain_of_ancestors):\n",
    "        for key, child_node in tree.items():\n",
    "            history.append(key)\n",
    "\n",
    "            if key == target:\n",
    "                chain_of_ancestors.append(key)\n",
    "                return True\n",
    "\n",
    "            result = dfs_recursive(target, child_node, history, chain_of_ancestors)\n",
    "\n",
    "            if result:\n",
    "                chain_of_ancestors.append(key)\n",
    "                return True\n",
    "\n",
    "        return False \n",
    "    \n",
    "    history = []\n",
    "    chain_of_ancestors = []\n",
    "    dfs_recursive(target, tree, history, chain_of_ancestors)\n",
    "    chain_of_ancestors.reverse()\n",
    "    return chain_of_ancestors, history \n",
    "  \n",
    "   \n",
    "    #raise NotImplementedError(\"Complete this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "arabic-filling",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_dfs():\n",
    "    tree = {\n",
    "      '1' : {'2':{}},\n",
    "      '3' : {'4':{}, '5':{}},\n",
    "      '6' : {'7': {'8': {'9':{}, '10': {}}}},\n",
    "      '11' : {} \n",
    "    }\n",
    "    \n",
    "    target = '9'\n",
    "    parents_ref = ['6', '7', '8', '9']\n",
    "    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    \n",
    "    assert dfs(target, tree) == (parents_ref, history_ref), \"Check you implementation for when node is in a tree!\"\n",
    "    \n",
    "    target = '111'\n",
    "    parents_ref = []\n",
    "    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
    "    \n",
    "    assert dfs(target, tree) == (parents_ref, history_ref), \"Check you implementation for when node can not be found!\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_dfs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-disclaimer",
   "metadata": {},
   "source": [
    "## Question 6: Breadth-first search (10 points)\n",
    "Now we are interested in counting the number of users at each \"level\" of the tree -- e.g., how many users joined on their own, how many users had one ancestor, how many users had two ancestors. For this purpose, we will use breadth-first search (BFS):\n",
    "1. Set current depth `k` to `0`.\n",
    "1. Get all nodes that are maximum depth `k` from the root node.\n",
    "1. Increase `k`.\n",
    "\n",
    "As an example, consider the same tree from Question 5:\n",
    "![](polytree.png)\n",
    "\n",
    "In this case, the BFS traversal would yield `[['1', '3', '6', '11'], ['2', '4', '5', '7'], ['8'], ['9', '10']]`. Note that this is a nested list, and every inner list is a collection of nodes at a certain level in the tree. Implement the function `bfs` that, given a referral database, returns the list of nodes at each level.\n",
    "\n",
    "**Notes**:\n",
    "* The order of the nodes within each inner list is not important; you can order them however you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "portable-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(tree):\n",
    "    '''\n",
    "    Traverses a tree with Breadth-First Search algorithm.\n",
    "    Args:\n",
    "        tree (Nested Dictionary) - tree to be traversed. \n",
    "                    Key:value relationship indicates parent:child nodes relationship.\n",
    "                    Empty set indicates leaf node.\n",
    "    Returns:\n",
    "        levels (List[List[String]]) - full traversal history. Each sublist is one full level with correct order.\n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "    queue = deque()\n",
    "  \n",
    "    queue.append(tree)\n",
    "    \n",
    "    while queue:\n",
    "        size = len(queue)\n",
    "        inner_list = []\n",
    "        for i in range(size):\n",
    "            subtree =  queue.popleft()\n",
    "\n",
    "            for parent, childnode in subtree.items():\n",
    "                inner_list.append(parent)\n",
    "                \n",
    "                if childnode:\n",
    "                    queue.append(childnode)\n",
    "\n",
    "        result.append(inner_list)\n",
    "\n",
    "    return result\n",
    "\n",
    "    #raise NotImplementedError(\"Complete this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "reliable-adolescent",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_bfs():\n",
    "\n",
    "    tree = {\n",
    "      '1' : {'2':{}},\n",
    "      '3' : {'4':{}, '5':{}},\n",
    "      '6' : {'7': {'8': {'9':{}, '10': {}}}},\n",
    "      '11' : {} \n",
    "    }\n",
    "    \n",
    "    levels_ref = [['1', '3', '6', '11'], ['2', '4', '5', '7'], ['8'], ['9', '10']]\n",
    "    levels_output = bfs(tree)\n",
    "    assert len(levels_ref) == len(levels_output)\n",
    "    for i in range(len(levels_ref)):\n",
    "        assert sorted(levels_ref[i]) == sorted(levels_output[i])\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_bfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-lewis",
   "metadata": {},
   "source": [
    "## Memory hierarchy and caching\n",
    "\n",
    "Now you have decided to add a bit of magic (or machine learning!) to your project and come up with a model that, given a user review text, yields the predicted rating for that review. You have decided to use this model on a stream of incoming data - namely, whenever you get some listing (which might be an existing or a new listing), predict the user rating for it.\n",
    "  \n",
    "Unfortunately, it appears that your magical model is processing each text rather slowly and while it does not matter for rare listings, it does matter for listings that appear often. One possible workaround is to store some of the previously computed ratings, so that they do not need to be computed again. By analyzing the resources on the expected devices, you found that you can have up to two storages - a \"hot\" storage, fast but with limited memory, and a \"cold\" one, slower but with more memory available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-delhi",
   "metadata": {},
   "source": [
    "### Question 7.1: Exploratory performance analysis (10 points)\n",
    "\n",
    "Below is how your magical model can be used: it accepts a string and outputs an integer value as predicted rating. In the scope of this project, you do not need to know how the rating is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "boxed-distinction",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "test_string = 'This course is great and I feel like I am learning a lot!'\n",
    "predicted_rating = predict(test_string)\n",
    "print(predicted_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-employer",
   "metadata": {},
   "source": [
    "Below is one way to create a hot storage and a cold storage.\n",
    "* The hot storage is a dictionary in RAM that simply maps each review text to its predicted rating. Then, given a review text, we can retrieve its predicted rating by getting the value which that text maps to in our dictionary.\n",
    "* The cold storage is a text file where each line stores a pair of hashed review text and predicted rating. This text file has more storage capabilities than what is available in RAM, but retrieving a review's rating takes longer because we need to loop through the file line by line. Note again that the review texts are hashed before being saved to the file, as a simple way of compressing data and saving some storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "indirect-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the hash function that is applied to every review text in the cold storage\n",
    "def hash_str(s):\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "colored-paraguay",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "class HotStorage:\n",
    "    def __init__(self):\n",
    "        self.cache_ram = {}\n",
    "    \n",
    "    def save_rating(self, review_text, predicted_rating):\n",
    "        self.cache_ram[review_text] = predicted_rating\n",
    "    \n",
    "    def get_rating(self, review_text):\n",
    "        return self.cache_ram.get(review_text)\n",
    "\n",
    "class ColdStorage:\n",
    "    def __init__(self, filename = \"cache_disk.txt\"):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def save_rating(self, review_text, predicted_rating):\n",
    "        hashed_text = hash_str(review_text)\n",
    "        with open(self.filename, \"a+\") as f:\n",
    "            f.write(hashed_text + \" \" + str(predicted_rating) + \"\\n\")\n",
    "    \n",
    "    def get_rating(self, review_text):\n",
    "        hashed_text = hash_str(review_text)\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                review_rating_pair = line.replace('\\n', '').split(' ')\n",
    "                if review_rating_pair[0] == hashed_text:\n",
    "                    return int(review_rating_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0128a4-5acd-4dd8-934f-3a0562cea180",
   "metadata": {},
   "source": [
    "Here is how we can save and retrieve a predicted rating from the two storages implemented above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "28630564-ff28-4c4d-aecb-2411dd676d70",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "hot_storage, cold_storage = HotStorage(), ColdStorage()\n",
    "hot_storage.save_rating(test_string, predicted_rating)\n",
    "cold_storage.save_rating(test_string, predicted_rating)\n",
    "\n",
    "print(hot_storage.get_rating(test_string))\n",
    "print(cold_storage.get_rating(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1589-f351-44b9-9e96-3355436cc7cb",
   "metadata": {},
   "source": [
    "Now that the set up is done, let's move on to evaluation. Assume the review text `test_str` comes up again in our stream of data, there are now three ways we can get its predicted rating:\n",
    "1. Use the `predict` function to recompute the predicted rating.\n",
    "1. Retrieve the predicted rating from the hot storage.\n",
    "1. Retrieve the predicted rating from the cold storage.\n",
    "\n",
    "Now it's your turn to write some code that implements each of the three options above and compares their runtime. The IPython magic command `%timeit` can be very useful for this task. Also make sure to tag every cell that has your experimental code with the tag `excluded_from_script`, so that they are ignored by the autograder; otherwise, IPython commands and print statements will interfere with the grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "aee713c7-7609-43c2-ad25-c11a4df3db1b",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ms ± 1.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "32.9 ns ± 0.721 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "28.7 µs ± 700 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "## your code here\n",
    "\n",
    "%timeit predict(test_string)\n",
    "%timeit hot_storage.save_rating(test_string, predicted_rating)\n",
    "%timeit cold_storage.save_rating(test_string, predicted_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-gnome",
   "metadata": {},
   "source": [
    "Based on your observations, replace `None` values in the function below with one of three strings: `\"compute\"`, `\"ram\"` or `\"disk\"`. `\"compute\"` refers to recomputing the predicted rating; `\"ram\"` refers to using the hot storage, and `\"disk\"` refers to using the cold storage. You can assume that the predicted rating is already present in both the hot and the cold storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "instrumental-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_hierarchy_selection():    \n",
    "    # Which way of getting the predicted rating is the fastest? \n",
    "    fastest_retrieval = \"ram\"\n",
    "    \n",
    "    # Which way of getting the predicted rating is the slowest? \n",
    "    slowest_retrieval = \"compute\"\n",
    "    \n",
    "    return fastest_retrieval, slowest_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "61703ffb-580b-4235-9c22-c82aae0e724a",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_data_hierarchy_selection():\n",
    "    fastest_retrieval, slowest_retrieval = data_hierarchy_selection()\n",
    "    assert fastest_retrieval in [\"compute\", \"ram\", \"disk\"]\n",
    "    assert slowest_retrieval in [\"compute\", \"ram\", \"disk\"]\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_data_hierarchy_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-cycling",
   "metadata": {},
   "source": [
    "All is well when there is only one predicted rating to retrieve. Unfortunately, AirBnB has a very large number of ratings, which cannot fit into either the hot or cold storage of the user's device. The workaround in this case is to use our available storages more selectively: we will only store values that are likely to come up again later. There is, of course, no way to know whether a value will come up again later, but a popular heuristic we can employ is that the more recent values are more likely to appear again and should be prioritized.\n",
    "\n",
    "To begin implementing this heuristic, first examine the `Cache` class below, which manages both a hot storage and a cold storage simultaneously. Its functionality is mostly taken from the classes `HotStorage` and `ColdStorage` from the previous question, but we also have functions that remove data from the storage. You do not need to modify anything in `Cache`, but will need to use this class for the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ordered-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    def __init__(self):\n",
    "        # make a copy of the initial cache file to be the working cache file\n",
    "        shutil.copy(\"cache_disk.txt\", \"working_cache_disk.txt\")\n",
    "        self.cache_ram = {}\n",
    "        self.cache_disk = \"working_cache_disk.txt\"\n",
    "        with open(self.cache_disk, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    \n",
    "    def get_from_ram_cache(self, text):\n",
    "        '''\n",
    "        Get value from RAM cache. \n",
    "        Args:\n",
    "            text - raw text for which predicted value is needed\n",
    "        Return:\n",
    "            Integer or None - predicted rating for text or None, \n",
    "                                if there requested text is not stored in this cache\n",
    "        '''\n",
    "        return self.cache_ram.get(text)\n",
    "    \n",
    "    def add_to_ram_cache(self, text, value):\n",
    "        '''\n",
    "        Add value to RAM cache. \n",
    "        Args:\n",
    "            text [String] - raw text\n",
    "            rating [Integer] - rating that should be saved\n",
    "        '''\n",
    "        self.cache_ram[text] = value\n",
    "    \n",
    "    def remove_from_ram_cache(self, text):\n",
    "        '''\n",
    "        Remove value from RAM cache. \n",
    "        Args:\n",
    "            text [String] - raw text\n",
    "        '''\n",
    "        del self.cache_ram[text]\n",
    "    \n",
    "    def ram_size(self):\n",
    "        '''\n",
    "        Get size of RAM cache. \n",
    "        Return:\n",
    "            Integer - current number of elements in RAM cache.\n",
    "        '''\n",
    "        return len(self.cache_ram)\n",
    "    \n",
    "    def get_from_disk_cache(self, text):\n",
    "        '''\n",
    "        Get value from disk cache. \n",
    "        Args:\n",
    "            text [String] - raw text for which predicted value is needed\n",
    "        Return:\n",
    "            Integer or None - predicted rating for text or None, \n",
    "                                if there requested text is not stored in this cache\n",
    "        '''\n",
    "        hashed_text = hash_str(text)\n",
    "        with open(self.cache_disk, \"r\") as f:\n",
    "            for line in f:\n",
    "                review_rating_pair = line.replace('\\n', '').split(' ')\n",
    "                if review_rating_pair[0] == hashed_text:\n",
    "                    return int(review_rating_pair[1])\n",
    "\n",
    "    def add_to_disk_cache(self, text, rating):\n",
    "        '''\n",
    "        Add value to disk cache. \n",
    "        Args:\n",
    "            text [String] - raw text\n",
    "            rating [Integer] - rating that should be saved\n",
    "        '''\n",
    "        key = hash_str(text)\n",
    "        with open(self.cache_disk, \"a+\") as f:\n",
    "            f.write(key + \" \" + str(rating) + \"\\n\")\n",
    "            \n",
    "    def remove_from_disk_cache(self, text):\n",
    "        '''\n",
    "        Remove value from disk cache. \n",
    "        Args:\n",
    "            text [String] - raw text\n",
    "        '''\n",
    "        hashed_text = hash_str(text)\n",
    "        with open(self.cache_disk, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        with open(self.cache_disk, \"w\") as f:\n",
    "            for line in lines:\n",
    "                review_rating_pair = line.replace('\\n', '').split(' ')\n",
    "                if review_rating_pair[0] != hashed_text:\n",
    "                    f.write(line)\n",
    "     \n",
    "    def disk_size(self):\n",
    "        '''\n",
    "        Get size of disk cache. \n",
    "        Return:\n",
    "            Integer - current number of elements in disk cache.\n",
    "        '''\n",
    "        i = -1\n",
    "        with open(self.cache_disk) as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        return i + 1\n",
    "    \n",
    "    def get(self, text):\n",
    "        '''\n",
    "        Compute the predicted rating by using the magical model\n",
    "        Args:\n",
    "            text  [String] - raw text\n",
    "        Return:\n",
    "            Integer - rating\n",
    "        '''\n",
    "        return predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-austria",
   "metadata": {},
   "source": [
    "### Question 7.2: \"Most recent\" caching strategy (10 points)\n",
    "\n",
    "Let's implement the form of caching that is based on temporal locality. We first denote `ram_max` and `disk_max` as the maximum number of elements that can be stored in RAM (hot storage) and in disk (cold storage) respectively. Here each element is a pair of review text and predicted rating. Given a new review text, which we denote as `new_text`, the caching algorithm works as follows:\n",
    "\n",
    "```\n",
    "If new_text is in the RAM cache, return its stored predicted rating, and mark this pair of (new_text, stored predicted rating) as the most recent element in RAM.\n",
    "If the hashed value of new_text is in the disk cache, return its stored predicted rating, and mark this pair of (hashed_new_text, stored predicted rating) as the most recent element in disk.\n",
    "If the above cases do not apply, compute the predicted rating of new_text, which we denote as predicted_rating. Then:\n",
    "    If the RAM cache's size is equal to ram_max:\n",
    "        If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.\n",
    "        Move the least recent element in the RAM cache to the disk cache (i.e., remove it from RAM cache and add it to disk cache).\n",
    "    Store the pair (text, predicted_value) in RAM cache\n",
    "    Update the most recent element in ram / disk and return predicted_rating \n",
    "```\n",
    "\n",
    "**Notes**:\n",
    "* Note that `MostRecentCache` inherits from `Cache`, so you are able to use or overwrite the methods in `Cache`. You are also free to modify the `Cache` class above, although doing so is not required.\n",
    "* You may need to initialize additional fields in the constructor to hold metadata about the RAM cache and disk cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "minimal-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostRecentCache(Cache):\n",
    "    def __init__(self, ram_max, disk_max):\n",
    "        '''\n",
    "        Args:\n",
    "            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage\n",
    "            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.ram_max = ram_max\n",
    "        self.disk_max = disk_max\n",
    "        self.recent_stack_ram = deque()\n",
    "        self.recent_stack_disk = deque() \n",
    "        \n",
    "        # initialize additional fields here\n",
    "        \n",
    "    def get(self, new_text):\n",
    " \n",
    "        ram_new_rating = self.get_from_ram_cache(new_text)\n",
    "        if ram_new_rating is not None:\n",
    "            self.recent_stack_ram.append(new_text)\n",
    "            return ram_new_rating\n",
    "        \n",
    "        disk_new_rating = self.get_from_disk_cache(new_text)\n",
    "        if disk_new_rating is not None:\n",
    "            self.recent_stack_disk.append(new_text)\n",
    "            return disk_new_rating\n",
    "            \n",
    "        compute_new_rating = super().get(new_text)\n",
    "        \n",
    "        if self.ram_size() >= self.ram_max:\n",
    "            if self.disk_size() >= self.disk_max:\n",
    "                recent_disk = self.recent_stack_disk.popleft()\n",
    "                self.remove_from_disk_cache(recent_disk)\n",
    " \n",
    "            recent_ram = self.recent_stack_ram.popleft()\n",
    "            move_rating = self.get_from_ram_cache(recent_ram)\n",
    "            self.add_to_disk_cache(recent_ram,move_rating)\n",
    "            self.recent_stack_disk.append(recent_ram)\n",
    "            self.remove_from_ram_cache(recent_ram)\n",
    "            \n",
    "        self.add_to_ram_cache(new_text, compute_new_rating)\n",
    "        self.recent_stack_ram.append(new_text)\n",
    "        return compute_new_rating\n",
    " \n",
    "    \n",
    "        # implement the caching algorithm\n",
    "        #raise NotImplementedError(\"Complete this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "incorporated-myanmar",
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_most_recent_cache():\n",
    "    c = MostRecentCache(5, 20)\n",
    "\n",
    "    for i in range(200):\n",
    "        c.get('test' + str(i))\n",
    "        \n",
    "    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], \"RAM cache incorrect!\"\n",
    "    \n",
    "    keys_disk = set()\n",
    "    with open(c.cache_disk, \"r\") as f:\n",
    "        for line in f:\n",
    "            li = line.replace('\\n', '').split(' ')\n",
    "            keys_disk.add(li[0])\n",
    "    \n",
    "    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',\n",
    "                         '06587144b7eefa0a96389e4edc25db495a24a2cc',\n",
    "                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',\n",
    "                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',\n",
    "                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',\n",
    "                         '2a373681c4cd01ca424fcc555dd834e417439ce9',\n",
    "                         '34d422df9f806e415383af86c4400ef7900c4b8f',\n",
    "                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',\n",
    "                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',\n",
    "                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',\n",
    "                         '899ec74db661e24fe58a4f6f302283acd4935ccf',\n",
    "                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',\n",
    "                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',\n",
    "                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',\n",
    "                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',\n",
    "                         'ad1c952a702edca89390aed15dda415100544632',\n",
    "                         'c34af7f961e92657a743b7390a76e0107c05abc8',\n",
    "                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',\n",
    "                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',\n",
    "                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, \"Disk cache incorrect!\"\n",
    "    \n",
    "    assert c.get('test') == 10, \"Prediction of new values is incorrect!\"\n",
    "    assert c.get('test196') == 9, \"Prediction of values from RAM is incorrect!\"\n",
    "    assert c.get('test178') == 9, \"Prediction of values from disk is incorrect!\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_most_recent_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1decc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
